{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92899fb1",
   "metadata": {},
   "source": [
    "## Task\n",
    "In this notebook you will work with the spark-nlp library to find some information about the the `body` column from the questions dataset.\n",
    "\n",
    "* spark-nlp [docs](https://nlp.johnsnowlabs.com/docs/en/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8cabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import size, col, sum, expr, explode, desc, length\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import SentenceDetector, Tokenizer, BertEmbeddings, NerDLModel, NerConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('NLP I')\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.2\")\n",
    "    .config(\"spark.executor.memory\", \"20g\")  # the memory is needed to run various parts of this notebook\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "data_input_path = os.path.join(project_path, 'data/questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77953507",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', data_input_path)\n",
    "    .load()\n",
    "    .withColumnRenamed('title', 'Text')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f86bbb",
   "metadata": {},
   "source": [
    "## Compute the number of sentences in the dataset.\n",
    "### Hint\n",
    "* use [documentAssembler](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/base/document_assembler/index.html) as the entry point in the Spark NLP lib\n",
    "* use [sentenceDetector](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/sentence/sentence_detector/index.html) to split the text into sentences\n",
    "* use [Pipeline](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html) to specify both steps and fit it on the DataFrame to create a model\n",
    "* use the model to transform the DataFrame. This will add a new column of array type to the dataframe\n",
    "* use [size](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.size.html#pyspark.sql.functions.size) to compute number of elements in the array\n",
    "* sum the size accross the entire DataFrame using [agg](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.agg.html#pyspark.sql.DataFrame.agg) and [sum](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sum.html#pyspark.sql.functions.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889e0e4",
   "metadata": {},
   "source": [
    "## Convert the `Text` column to tokens\n",
    "### Hint\n",
    "* use [Tokenizer](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/token/tokenizer/index.html)\n",
    "* use [Pipeline](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html) and set the stages with the documentAssebler and Tokenizer\n",
    "* fit the pipeline on the DataFrame to create a model\n",
    "* use the model to transform the DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ba02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c9ce92",
   "metadata": {},
   "source": [
    "## Compute embedings for the tokens.\n",
    "### Hint\n",
    "* use pretrained bert model called `bert_base_cased` using [BertEmbeddings](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/embeddings/bert_embeddings/index.html) by calling BertEmbeddings.pretrained('bert_base_cased', 'en')\n",
    "* then define the Pipeline as in previous questions and add the embedding as another stage to create new model\n",
    "* finaly use the model to transform the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ad284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9641c",
   "metadata": {},
   "source": [
    "## Compute NER (Named Entity Recognition)\n",
    "### Hint\n",
    "* use a pretrained [NerDLModel](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/ner/ner_dl/index.html#sparknlp.annotator.ner.ner_dl.NerDLModel) model.\n",
    "* Use specifically `ner_conll_bert_base_cased` which is compatible with the `bert_base_cased` embedding we computed in the previous question. This should be specified as the argument to the pretrained method `NerDLModel.pretrained('ner_conll_bert_base_cased', 'en')`\n",
    "* the display function may fail for displaying the embedings since they are large. In that case use show() instead which will truncate the output by default. (You can also drop the embeddings column for the display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e312d",
   "metadata": {},
   "source": [
    "## Extract the entities from the result and find entities that are the most frequent.\n",
    "### Hint\n",
    "* use [NerConverter](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/ner/ner_converter/index.html#sparknlp.annotator.ner.ner_converter.NerConverter) as another step in the pipeline. It will convert the NER output to more friendly representation.\n",
    "* fit again the pipeline and transform the DataFrame\n",
    "* filter only for rows where the output is not empty using [size](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.size.html#pyspark.sql.functions.size)('entities') > 0\n",
    "* use higher order function [TRANSFORM](https://spark.apache.org/docs/latest/api/sql/index.html#transform) to extract `result` and `entity` fields from the `entities` array\n",
    "* [explode](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.explode.html#pyspark.sql.functions.explode) the final array\n",
    "* finaly group by entity and count number of occurences and sort the the result in descending order using [orderBy](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html#pyspark.sql.DataFrame.orderBy) and [desc](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.desc.html#pyspark.sql.functions.desc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24075ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
